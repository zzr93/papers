# The Google File System

## 摘要

GFS是面向大规模分布式数据密集型应用所设计的可扩展的分布式文件系统。其特点包含：

* 运行在低价普通硬件上
* 有容灾能力
* 为海量客户端提供性能优秀的服务

GFS面向谷歌的业务负载，其设计假设与早期分布式文件系统有很大差别。GFS很好的满足了谷歌内部应用对超大数据集的需求，当前最大的GFS集群有几百TB的数据量，这些数据分散在一千台机器的几千块磁盘上，并同时向几百个客户端提供服务。这篇文章会阐述GFS的设计以及benchmark。

## 正文

### 1. Introduction

GFS在性能、扩展性、可靠性、可用性上与传统分布式存储目标一致。不同之处在于GFS要面向谷歌应用负载及科学实验设计，这样的前提带来以下设计假设：

* **组件经常故障**。一个在低价硬件上运行的庞大系统每时每刻都会有故障发生，且很多故障无法短时间内恢复。这些故障包含但不限于应用bug、操作系统bug、人为错误操作、硬盘/内存/连接器/网络故障、甚至电力供给。因此，持续的监控、错误检测、容灾、自恢复能力是这个系统能稳定运行的基础。
* **大文件**。几GB的文件很常见，每个文件中通常都有很多对象（例如一个网页），把它们聚合成一个个大文件管理相比管理许多小文件更加明智。
* **写操作主要为顺序追加写**。几乎没有随意写。文件写完，后续基本只有读和追加写操作。这类文件主要有：数据分析程序扫描的大型数据集、app产生的数据流、归档数据、实时数据
* **应用与文件系统接口协同设计**。GFS有着宽松的一致性模型。多个client可以并发向同个文件中追加写

### 2. Design Overview

#### 2.1 Assumptions

* 系统运行在经常故障的低价硬件上。因此系统要能持续的监控发现故障、能容灾、能自恢复
* 系统存储几百万规模的大文件。100MB-几GB的文件都很常见。小文件也允许进入系统，但不必为之优化
* 两种读：
  1. 流式大量读。单请求通常读几百KB-几MB，同个client通常会持续将文件的某个区域读完
  2. 随机小量读。单请求通常读几KB。性能敏感的应用通常会自己做些优化，例如排序后批量读
* 顺序大量写。单请求通常几百KB-几MB。随机写也被允许，但不做优化
* 为多客户端并发写同个文件提供支持。需要在保持原子性的同时降低同步带来的消耗
* 考虑吞吐优先于时延

#### 2.2 Interface

GFS的文件按目录结构组织，用路径索引，提供create/delete/open/close/read/write这些基本接口。GFS还提供了Snapshot与Record append接口。Snapshot可以低成本拷贝一个文件